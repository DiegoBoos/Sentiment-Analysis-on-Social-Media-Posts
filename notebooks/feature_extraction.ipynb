{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features saved to ../data/feature_extraction/TFIDF_Features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'../data/preprocessing/preprocessed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "df['Processed_Title'].fillna('', inplace=True)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000,  # Larger number of features given the dataset size\n",
    "                                   ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "                                   min_df=3,  # Lower min_df since dataset isn't very large\n",
    "                                   max_df=0.85,  # Exclude terms that are too common\n",
    "                                   sublinear_tf=True)  # Apply sublinear scaling\n",
    "\n",
    "# Apply the vectorizer to the processed titles\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Processed_Title'])\n",
    "\n",
    "# Convert the matrix to a DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Optionally, save the matrix to a CSV file\n",
    "output_file = r'../data/feature_extraction/TFIDF_Features.csv'\n",
    "tfidf_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"TF-IDF features saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your dataset\n",
    "file_path = r'../data/preprocessing/preprocessed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "df['Processed_Title'].fillna('', inplace=True)\n",
    "\n",
    "# Tokenize your preprocessed text\n",
    "tokenized_text = [text.split() for text in df['Processed_Title']]\n",
    "\n",
    "# Create and train the Word2Vec model\n",
    "word2vec_model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Define a function to average word vectors for a text\n",
    "def document_vector(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv]\n",
    "    if not doc:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Apply the function to each document\n",
    "df['Doc_Vector'] = [document_vector(doc) for doc in tokenized_text]\n",
    "\n",
    "# Split the vectors into their own columns for CSV output\n",
    "vector_df = pd.DataFrame(df['Doc_Vector'].tolist())\n",
    "\n",
    "# Concatenate the original dataframe with the vector dataframe\n",
    "df = pd.concat([df, vector_df], axis=1)\n",
    "\n",
    "# Drop the 'Doc_Vector' column\n",
    "df.drop('Doc_Vector', axis=1, inplace=True)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "output_file_path = r'../data/feature_extraction/word_vectors.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'../data/preprocessing/preprocessed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace NaN values in 'Processed_Title' column with empty string\n",
    "df['Processed_Title'].fillna(\"\", inplace=True)\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to encode text using BERT\n",
    "def bert_encode(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy() # bert model will contain the inputs and the output will be the last hidden state\n",
    "\n",
    "# Apply BERT encoding to each item in 'Processed_Title'\n",
    "bert_embeddings = df['Processed_Title'].apply(bert_encode)\n",
    "\n",
    "# Convert embeddings to a DataFrame\n",
    "bert_embeddings_df = pd.DataFrame([embedding[0] for embedding in bert_embeddings])\n",
    "\n",
    "# Concatenate original dataframe with embeddings\n",
    "final_df = pd.concat([df, bert_embeddings_df], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "output_file_path = r'../data/feature_extraction/BERT_Features.csv'\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"BERT features dataset saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
