{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated dataset saved to ../data/data_collection/annotated_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the dataset\n",
    "csv_file_path = r'../data/data_collection/reddit.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Load the sentiment analysis model\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "def annotate_sentiment(text):\n",
    "    try:\n",
    "        # Get the result from the classifier\n",
    "        result = classifier(text)\n",
    "        # Get the label and score\n",
    "        label = result[0]['label'].lower()\n",
    "        score = result[0]['score']\n",
    "\n",
    "        # Set thresholds for determining 'neutral' sentiment\n",
    "        neutral_threshold_lower = 0.4\n",
    "        neutral_threshold_upper = 0.6\n",
    "\n",
    "        # If the score is not confident, label as 'neutral'\n",
    "        if neutral_threshold_lower <= score <= neutral_threshold_upper:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return 'neutral'\n",
    "\n",
    "# Annotate each post with a sentiment label\n",
    "df['Sentiment'] = df['Title'].apply(annotate_sentiment)\n",
    "\n",
    "# Save the annotated dataset to a new CSV file\n",
    "annotated_file_path = r'../data/data_collection/annotated_dataset.csv'\n",
    "df.to_csv(annotated_file_path, index=False)\n",
    "\n",
    "print(f\"Annotated dataset saved to {annotated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "negative    3270\n",
      "positive    2221\n",
      "neutral      156\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the annotated CSV file\n",
    "annotated_df = pd.read_csv('../data/data_collection/annotated_dataset.csv')\n",
    "\n",
    "# Count the number of occurrences of each sentiment label\n",
    "sentiment_counts = annotated_df['Sentiment'].value_counts()\n",
    "\n",
    "# Print the count of each sentiment\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/diegobolanos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/diegobolanos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/diegobolanos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df_annotated = pd.read_csv('../data/data_collection/annotated_dataset.csv')\n",
    "\n",
    "# Slang dictionary\n",
    "slang_dict = {\n",
    "    'btw': 'by the way',\n",
    "    'lol': 'laughing out loud',\n",
    "    'idk': 'I do not know',\n",
    "    'imo': 'in my opinion',\n",
    "    'imho': 'in my humble opinion',\n",
    "    'brb': 'be right back',\n",
    "    'tbh': 'to be honest',\n",
    "    'lmao': 'laughing my ass off',\n",
    "    'rofl': 'rolling on the floor laughing',\n",
    "    'smh': 'shaking my head',\n",
    "    'omg': 'oh my god',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'afaik': 'as far as I know',\n",
    "    'irl': 'in real life',\n",
    "    'thx': 'thanks',\n",
    "    'pls': 'please',\n",
    "    'dm': 'direct message',\n",
    "    'fyi': 'for your information',\n",
    "    'b4': 'before',\n",
    "    'gr8': 'great',\n",
    "    'u': 'you',\n",
    "    'r': 'are',\n",
    "    'yolo': 'you only live once',\n",
    "    'np': 'no problem',\n",
    "    'g2g': 'got to go',\n",
    "    'tldr': 'too long, didnâ€™t read',\n",
    "    'jk': 'just kidding',\n",
    "    'bff': 'best friends forever',\n",
    "    'icymi': 'in case you missed it',\n",
    "    'fomo': 'fear of missing out',\n",
    "    'ftw': 'for the win',\n",
    "    'wtf': 'what the f***',\n",
    "    'nsfw': 'not safe for work',\n",
    "    'nbd': 'no big deal',\n",
    "    'faq': 'frequently asked questions',\n",
    "    'afk': 'away from keyboard',\n",
    "    'asap': 'as soon as possible'\n",
    "}\n",
    "\n",
    "# Function to extract and process hashtags\n",
    "def process_hashtags(text):\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "    return ' '.join(hashtags)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert emojis to text and process hashtags\n",
    "    text = demojize(text) + ' ' + process_hashtags(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Translate slang\n",
    "    tokens = [slang_dict.get(token, token) for token in tokens]\n",
    "\n",
    "    # Remove non-alphabetic characters and keep the tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated['num_words'] = df_annotated['Title'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of words in the longest post later use \n",
    "df_annotated.num_words.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each title\n",
    "df_annotated['Processed_Title'] = df_annotated['Title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 5647 entries, 0 to 5646\n",
      "Series name: Sentiment\n",
      "Non-Null Count  Dtype   \n",
      "--------------  -----   \n",
      "5647 non-null   category\n",
      "dtypes: category(1)\n",
      "memory usage: 5.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Apply encoding to each sentiment\n",
    "df_annotated['Sentiment'] = df_annotated['Sentiment'].astype('category')\n",
    "df_annotated['Sentiment'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive', 'neutral']\n",
       "Categories (3, object): ['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotated['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated['Sentiment'] = df_annotated['Sentiment'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Author</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>num_words</th>\n",
       "      <th>Processed_Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas Supreme Court Rules Against Woman Who So...</td>\n",
       "      <td>news</td>\n",
       "      <td>Lifeboatb</td>\n",
       "      <td>7231</td>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>texas supreme court rule woman sought abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Texas woman who sought court permission for ab...</td>\n",
       "      <td>news</td>\n",
       "      <td>11-110011</td>\n",
       "      <td>24560</td>\n",
       "      <td>2327</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>texas woman sought court permission abortion l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home Alone star gets life-saving cancer surger...</td>\n",
       "      <td>news</td>\n",
       "      <td>ILikeTalkn2Myself</td>\n",
       "      <td>8341</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>home alone star get cancer surgery raised online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russian opposition leader Navalny missing from...</td>\n",
       "      <td>news</td>\n",
       "      <td>agnesiswitch</td>\n",
       "      <td>12825</td>\n",
       "      <td>742</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>russian opposition leader navalny missing pris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Special counsel goes directly to Supreme Court...</td>\n",
       "      <td>news</td>\n",
       "      <td>hamsterberry</td>\n",
       "      <td>5294</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>special counsel go directly supreme court reso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Subreddit   \n",
       "0  Texas Supreme Court Rules Against Woman Who So...      news  \\\n",
       "1  Texas woman who sought court permission for ab...      news   \n",
       "2  Home Alone star gets life-saving cancer surger...      news   \n",
       "3  Russian opposition leader Navalny missing from...      news   \n",
       "4  Special counsel goes directly to Supreme Court...      news   \n",
       "\n",
       "              Author  Likes  Comments  Sentiment  num_words   \n",
       "0          Lifeboatb   7231       912          0         10  \\\n",
       "1          11-110011  24560      2327          0         15   \n",
       "2  ILikeTalkn2Myself   8341       499          0         11   \n",
       "3       agnesiswitch  12825       742          0         10   \n",
       "4       hamsterberry   5294       484          0         15   \n",
       "\n",
       "                                     Processed_Title  \n",
       "0     texas supreme court rule woman sought abortion  \n",
       "1  texas woman sought court permission abortion l...  \n",
       "2   home alone star get cancer surgery raised online  \n",
       "3  russian opposition leader navalny missing pris...  \n",
       "4  special counsel go directly supreme court reso...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to ../data/preprocessing/preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file path to save the preprocessed dataset\n",
    "preprocessed_file_path = '../data/preprocessing/preprocessed.csv'\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "df_annotated.to_csv(preprocessed_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed dataset saved to {preprocessed_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
